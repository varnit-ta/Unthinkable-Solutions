"""
AI Ingredient Detection Service
Fast, local ingredient detection using Salesforce BLIP and OpenAI CLIP models
"""
import io
import logging
import re
from typing import List, Dict, Any, Set
from PIL import Image
from fastapi import FastAPI, File, UploadFile, HTTPException
from fastapi.middleware.cors import CORSMiddleware
import torch
from transformers import BlipProcessor, BlipForConditionalGeneration, CLIPProcessor, CLIPModel
import uvicorn

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

app = FastAPI(
    title="AI Ingredient Detection Service",
    description="Local AI service for detecting ingredients from images using BLIP and CLIP",
    version="2.0.0"
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "DELETE", "OPTIONS"],
    allow_headers=["*"],
)

blip_processor = None
blip_model = None

clip_processor = None
clip_model = None

device = None

INGREDIENTS_LIST = [
    # Vegetables
    "tomato", "onion", "garlic", "potato", "carrot", "bell pepper", "chili pepper",
    "ginger", "lettuce", "cabbage", "spinach", "mushroom", "broccoli", "cauliflower",
    "cucumber", "zucchini", "eggplant", "corn", "peas", "beans", "celery", "leek",
    "asparagus", "radish", "turnip", "beet", "pumpkin", "squash", "kale", "chard",
    # Proteins
    "chicken", "beef", "pork", "lamb", "fish", "salmon", "tuna", "shrimp", "prawns",
    "crab", "lobster", "egg", "tofu", "tempeh", "bacon", "sausage", "ham",
    # Dairy
    "cheese", "mozzarella", "cheddar", "parmesan", "milk", "butter", "cream", "yogurt",
    # Grains & Carbs
    "rice", "pasta", "noodles", "bread", "flour", "quinoa", "couscous", "oats",
    # Fruits
    "lemon", "lime", "apple", "banana", "orange", "avocado", "mango", "pineapple",
    "strawberry", "blueberry", "tomato", "coconut",
    # Herbs & Spices
    "basil", "parsley", "cilantro", "coriander", "mint", "thyme", "rosemary", "oregano",
    "dill", "sage", "bay leaf", "cumin", "paprika", "turmeric", "curry",
    # Condiments & Oils
    "olive oil", "vegetable oil", "soy sauce", "vinegar", "salt", "pepper", "sugar",
    "honey", "ketchup", "mustard", "mayonnaise",
    # Nuts & Seeds
    "almond", "cashew", "peanut", "walnut", "sesame seeds", "sunflower seeds",
]

CUISINE_LIST = [
    "Italian", "Chinese", "Indian", "Mexican", "Thai", "Japanese",
    "French", "Mediterranean", "American", "Korean", "Vietnamese",
    "Greek", "Spanish", "Middle Eastern", "Turkish", "Lebanese"
]

DISH_TYPES = [
    "salad", "soup", "curry", "pasta", "stir fry", "grilled meat", "roasted vegetables",
    "sandwich", "pizza", "burger", "stew", "casserole", "rice dish", "noodle dish",
    "seafood dish", "dessert", "breakfast", "appetizer", "main course"
]


def load_model():
    """Load the Salesforce BLIP and OpenAI CLIP models on startup"""
    global blip_processor, blip_model, clip_processor, clip_model, device
    
    try:
        device = "cuda" if torch.cuda.is_available() else "cpu"
        logger.info(f"Using device: {device}")
        
        # Load BLIP for captioning
        logger.info("Loading Salesforce BLIP model for captioning...")
        blip_model_name = "Salesforce/blip-image-captioning-base"
        blip_processor = BlipProcessor.from_pretrained(blip_model_name)
        blip_model = BlipForConditionalGeneration.from_pretrained(blip_model_name).to(device)
        logger.info("BLIP model loaded successfully!")
        
        # Load CLIP for ingredient detection
        logger.info("Loading OpenAI CLIP model for ingredient detection...")
        clip_model_name = "openai/clip-vit-base-patch32"
        clip_processor = CLIPProcessor.from_pretrained(clip_model_name)
        clip_model = CLIPModel.from_pretrained(clip_model_name).to(device)
        logger.info("CLIP model loaded successfully!")
        
        logger.info("All models loaded successfully!")
    except Exception as e:
        logger.error(f"Failed to load models: {str(e)}", exc_info=True)
        raise


def extract_ingredients_from_caption(caption: str) -> List[str]:
    """
    Extract individual ingredients from the generated caption
    
    Args:
        caption: Image caption generated by BLIP model
    
    Returns:
        List of detected ingredients
    """
    stop_words = {
        'a', 'an', 'the', 'and', 'or', 'of', 'in', 'on', 'with', 'at', 'to',
        'photo', 'picture', 'image', 'plate', 'bowl', 'table', 'dish', 'food',
        'kitchen', 'counter', 'cutting', 'board', 'wooden', 'white', 'black',
        'fresh', 'some', 'many', 'few', 'several', 'various', 'different'
    }
    
    ingredient_mapping = {
        'tomato': ['tomato', 'tomatoes'],
        'onion': ['onion', 'onions'],
        'garlic': ['garlic'],
        'potato': ['potato', 'potatoes'],
        'carrot': ['carrot', 'carrots'],
        'pepper': ['pepper', 'peppers', 'bell pepper'],
        'chili': ['chili', 'chilis', 'chilli'],
        'ginger': ['ginger'],
        'lettuce': ['lettuce'],
        'cabbage': ['cabbage'],
        'spinach': ['spinach'],
        'mushroom': ['mushroom', 'mushrooms'],
        'broccoli': ['broccoli'],
        'cauliflower': ['cauliflower'],
        'cucumber': ['cucumber', 'cucumbers'],
        'zucchini': ['zucchini'],
        'eggplant': ['eggplant', 'eggplants', 'aubergine'],
        'chicken': ['chicken'],
        'beef': ['beef'],
        'pork': ['pork'],
        'fish': ['fish'],
        'shrimp': ['shrimp', 'prawns'],
        'egg': ['egg', 'eggs'],
        'cheese': ['cheese'],
        'milk': ['milk'],
        'butter': ['butter'],
        'rice': ['rice'],
        'pasta': ['pasta'],
        'bread': ['bread'],
        'flour': ['flour'],
        'oil': ['oil'],
        'salt': ['salt'],
        'sugar': ['sugar'],
        'lemon': ['lemon', 'lemons'],
        'lime': ['lime', 'limes'],
        'apple': ['apple', 'apples'],
        'banana': ['banana', 'bananas'],
        'orange': ['orange', 'oranges'],
        'avocado': ['avocado', 'avocados'],
        'corn': ['corn'],
        'bean': ['bean', 'beans'],
        'pea': ['pea', 'peas'],
        'basil': ['basil'],
        'parsley': ['parsley'],
        'cilantro': ['cilantro', 'coriander'],
        'mint': ['mint'],
        'thyme': ['thyme'],
        'rosemary': ['rosemary'],
    }
    
    caption_lower = caption.lower()
    words = re.findall(r'\b[a-z]+\b', caption_lower)
    
    detected_ingredients: Set[str] = set()
    
    for base_ingredient, variants in ingredient_mapping.items():
        for variant in variants:
            if variant in caption_lower:
                detected_ingredients.add(base_ingredient)
                break
    
    if not detected_ingredients:
        for word in words:
            if word not in stop_words and len(word) > 3:
                detected_ingredients.add(word)
    
    return sorted(list(detected_ingredients))


def detect_with_clip(image: Image.Image) -> Dict[str, Any]:
    """
    Use CLIP to detect ingredients, cuisine, and dish type
    
    Args:
        image: PIL Image object
    
    Returns:
        Dictionary with detected ingredients, cuisine, and dish type
    """
    results = {}
    
    try:
        logger.info("CLIP: Detecting ingredients...")
        inputs = clip_processor(text=INGREDIENTS_LIST, images=image, return_tensors="pt", padding=True).to(device)
        outputs = clip_model(**inputs)
        logits_per_image = outputs.logits_per_image
        probs = logits_per_image.softmax(dim=1)[0]
        
        ingredient_scores = [(INGREDIENTS_LIST[i], float(probs[i])) for i in range(len(INGREDIENTS_LIST))]
        ingredient_scores.sort(key=lambda x: x[1], reverse=True)
        
        detected_ingredients = [name for name, score in ingredient_scores if score > 0.20]
        ingredient_confidences = {name: score for name, score in ingredient_scores if score > 0.20}
        
        logger.info(f"CLIP: Detected {len(detected_ingredients)} ingredients")
        
        logger.info("CLIP: Detecting cuisine...")
        inputs = clip_processor(text=CUISINE_LIST, images=image, return_tensors="pt", padding=True).to(device)
        outputs = clip_model(**inputs)
        cuisine_probs = outputs.logits_per_image.softmax(dim=1)[0]
        cuisine_idx = int(cuisine_probs.argmax())
        cuisine_confidence = float(cuisine_probs[cuisine_idx])
        
        logger.info("CLIP: Detecting dish type...")
        inputs = clip_processor(text=DISH_TYPES, images=image, return_tensors="pt", padding=True).to(device)
        outputs = clip_model(**inputs)
        dish_probs = outputs.logits_per_image.softmax(dim=1)[0]
        dish_idx = int(dish_probs.argmax())
        dish_confidence = float(dish_probs[dish_idx])
        
        results = {
            "ingredients": detected_ingredients[:10],
            "ingredient_confidences": ingredient_confidences,
            "cuisine": CUISINE_LIST[cuisine_idx],
            "cuisine_confidence": cuisine_confidence,
            "dish_type": DISH_TYPES[dish_idx],
            "dish_confidence": dish_confidence,
        }
        
        logger.info(f"CLIP: Cuisine={results['cuisine']}, Dish={results['dish_type']}")
        
    except Exception as e:
        logger.error(f"CLIP detection error: {str(e)}", exc_info=True)
        results = {
            "ingredients": [],
            "ingredient_confidences": {},
            "cuisine": "Unknown",
            "cuisine_confidence": 0.0,
            "dish_type": "Unknown",
            "dish_confidence": 0.0,
        }
    
    return results


@app.on_event("startup")
async def startup_event():
    """Load model when server starts"""
    load_model()


@app.get("/")
async def root():
    """Health check endpoint"""
    return {
        "service": "AI Ingredient Detection",
        "status": "running",
        "model": "Salesforce/blip-image-captioning-base",
        "device": str(device)
    }


@app.get("/health")
async def health():
    """Detailed health check"""
    return {
        "status": "healthy",
        "blip_model_loaded": blip_model is not None,
        "blip_processor_loaded": blip_processor is not None,
        "clip_model_loaded": clip_model is not None,
        "clip_processor_loaded": clip_processor is not None,
        "device": str(device)
    }


@app.post("/detect")
@app.post("/detect-ingredients")
async def detect_ingredients(file: UploadFile = File(...)):
    """
    Detect ingredients from an uploaded image using both BLIP and CLIP
    
    Args:
        file: Uploaded image file (JPEG, PNG, etc.)
    
    Returns:
        JSON with detected ingredients list, cuisine, dish type, caption, and confidence
    """
    try:
        logger.info(f"Received request - filename: {file.filename}, content_type: {file.content_type}")
        
        if blip_model is None or blip_processor is None or clip_model is None or clip_processor is None:
            logger.error("Models not loaded!")
            raise HTTPException(
                status_code=503,
                detail="Models are still loading. Please wait a moment and try again."
            )
        
        if not file.content_type or not file.content_type.startswith('image/'):
            raise HTTPException(
                status_code=400,
                detail=f"Invalid file type: {file.content_type}. Must be an image."
            )
        
        logger.info(f"Processing image: {file.filename}")
        contents = await file.read()
        logger.info(f"Read {len(contents)} bytes")
        
        if len(contents) == 0:
            raise HTTPException(
                status_code=400,
                detail="Empty image file received."
            )
        
        logger.info("Opening image...")
        image = Image.open(io.BytesIO(contents)).convert('RGB')
        logger.info(f"Image opened successfully - size: {image.size}")
        
        # Use CLIP for structured detection
        logger.info("Running CLIP detection...")
        clip_results = detect_with_clip(image)
        
        # Use BLIP for caption generation
        logger.info("Generating caption with BLIP...")
        inputs = blip_processor(image, return_tensors="pt").to(device)
        
        with torch.no_grad():
            out = blip_model.generate(**inputs, max_length=50)
        
        caption = blip_processor.decode(out[0], skip_special_tokens=True)
        logger.info(f"Generated caption: {caption}")
        
        ingredients = clip_results["ingredients"]
        
        if len(ingredients) < 3:
            logger.info("Supplementing with caption-based ingredients...")
            caption_ingredients = extract_ingredients_from_caption(caption)
            for ing in caption_ingredients:
                if ing not in ingredients:
                    ingredients.append(ing)
        
        logger.info(f"Final ingredients: {ingredients}")
        
        avg_confidence = (
            clip_results["cuisine_confidence"] + 
            clip_results["dish_confidence"]
        ) / 2.0
        
        response = {
            "success": True,
            "ingredients": ingredients,
            "cuisine": clip_results["cuisine"],
            "dish_type": clip_results["dish_type"],
            "caption": caption,
            "confidence": round(avg_confidence, 2),
            "details": {
                "ingredient_confidences": clip_results["ingredient_confidences"],
                "cuisine_confidence": round(clip_results["cuisine_confidence"], 2),
                "dish_confidence": round(clip_results["dish_confidence"], 2),
            },
            "model": {
                "clip": "openai/clip-vit-base-patch32",
                "blip": "Salesforce/blip-image-captioning-base"
            },
            "device": str(device)
        }
        
        logger.info(f"Successfully processed image with {len(ingredients)} ingredients, cuisine: {clip_results['cuisine']}, dish: {clip_results['dish_type']}")
        return response
        
    except HTTPException as he:
        logger.error(f"HTTP Exception: {he.detail}")
        raise
    except Exception as e:
        error_msg = f"{type(e).__name__}: {str(e)}"
        logger.error(f"Error processing image: {error_msg}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail=f"Error processing image: {error_msg}"
        )


@app.post("/detect-batch")
async def detect_ingredients_batch(files: List[UploadFile] = File(...)):
    """
    Detect ingredients from multiple images
    
    Args:
        files: List of uploaded image files
    
    Returns:
        JSON with results for each image
    """
    results = []
    
    for file in files:
        try:
            result = await detect_ingredients(file)
            results.append({
                "filename": file.filename,
                "result": result
            })
        except Exception as e:
            results.append({
                "filename": file.filename,
                "error": str(e)
            })
    
    return {
        "success": True,
        "count": len(results),
        "results": results
    }


if __name__ == "__main__":
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=8000,
        reload=False,
        log_level="info"
    )
