# ğŸ¤– AI Ingredient Detection Service

A local Python microservice that provides AI-powered ingredient detection using the Salesforce BLIP image captioning model.

## Features

- ğŸš€ **Fast & Local** - No external API calls, runs on your machine
- ğŸ”“ **No API Keys** - No rate limits or quotas
- ğŸ¯ **Accurate** - Uses Salesforce BLIP, one of the best image captioning models
- ğŸ³ **Containerized** - Runs seamlessly with Docker Compose
- âš¡ **GPU Support** - Automatically uses GPU if available (CUDA)

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  Go Backend  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ Python AI    â”‚
â”‚   (React)   â”‚         â”‚              â”‚         â”‚   Service    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             HTTP                      HTTP
                        /detect-ingredients         /detect
                                                   (port 8000)
```

## How It Works

1. **User uploads image** â†’ Frontend sends to Go backend
2. **Go backend** â†’ Forwards image to Python AI service
3. **Python AI service**:
   - Loads image with PIL
   - Processes with BLIP model
   - Generates caption: "a photo of tomatoes, onions, and garlic"
4. **Go backend** â†’ Parses caption to extract ingredients
5. **Response** â†’ Returns `["tomato", "onion", "garlic"]` to frontend

## API Endpoints

### `GET /`
Health check - Returns service status and model info

**Response:**
```json
{
  "service": "AI Ingredient Detection",
  "status": "running",
  "model": "Salesforce/blip-image-captioning-base",
  "device": "cpu"
}
```

### `GET /health`
Detailed health check

**Response:**
```json
{
  "status": "healthy",
  "model_loaded": true,
  "processor_loaded": true,
  "device": "cpu"
}
```

### `POST /detect`
Detect ingredients from an image

**Request:**
- Content-Type: `multipart/form-data`
- Field: `file` (image file - JPEG, PNG, etc.)

**Response:**
```json
{
  "success": true,
  "caption": "a photo of tomatoes, onions, and garlic on a table",
  "confidence": 0.85,
  "model": "Salesforce/blip-image-captioning-base",
  "device": "cpu"
}
```

## Running the Service

### With Docker Compose (Recommended)

```bash
# Start all services including AI service
docker-compose up --build

# AI service will be available at http://localhost:8000
```

### Standalone (For Development)

```bash
cd ai-service

# Install dependencies
pip install -r requirements.txt

# Run the service
python main.py

# Service runs on http://localhost:8000
```

## Testing

### Test with curl

```bash
# Health check
curl http://localhost:8000/health

# Test ingredient detection
curl -X POST http://localhost:8000/detect \
  -F "file=@path/to/your/image.jpg"
```

### Test in browser

Visit http://localhost:8000 to see service status

## Model Information

- **Model**: Salesforce/blip-image-captioning-base
- **Size**: ~990MB (downloaded on first run)
- **Cache Location**: `~/.cache/huggingface/`
- **Performance**: 
  - CPU: ~2-5 seconds per image
  - GPU: ~0.5-1 second per image

## GPU Support

The service automatically detects and uses GPU if available:

```bash
# Check if GPU is being used
docker-compose logs ai-service | grep device

# Should show: "Using device: cuda" if GPU available
# Or: "Using device: cpu" if running on CPU
```

## Environment Variables

None required! The service works out of the box.

Optional:
- Model is downloaded automatically on first run
- Cache is persisted in Docker volume

## Troubleshooting

### Service won't start
- Check Docker is running: `docker ps`
- Check logs: `docker-compose logs ai-service`

### "Model is loading"
- First startup takes 1-2 minutes to download the model (~990MB)
- Subsequent starts are faster (model is cached)

### Slow inference
- Service runs on CPU by default
- For faster inference, use a machine with GPU support
- CPU inference takes 2-5 seconds per image (acceptable for most use cases)

### Out of memory
- BLIP model requires ~2GB RAM
- If running on limited resources, service may crash
- Solution: Increase Docker memory limit or use a larger instance

## Development

### Project Structure

```
ai-service/
â”œâ”€â”€ main.py           # FastAPI server with BLIP model
â”œâ”€â”€ requirements.txt  # Python dependencies
â”œâ”€â”€ Dockerfile       # Container definition
â””â”€â”€ README.md        # This file
```

### Adding Features

Want to enhance the service? Here are some ideas:

1. **Batch Processing** - Already supported via `/detect-batch` endpoint
2. **Different Models** - Easy to swap BLIP for other models
3. **Caching** - Add Redis to cache results
4. **Preprocessing** - Add image quality checks
5. **Logging** - Enhanced logging with file output

## Performance

Tested on various setups:

| Setup | Inference Time | Startup Time |
|-------|---------------|--------------|
| CPU (4 cores) | 3-5 seconds | 30 seconds |
| GPU (CUDA) | 0.5-1 second | 45 seconds |
| Apple M1 | 2-3 seconds | 30 seconds |

## License

Same as parent project - see root LICENSE file.

## Questions?

Open an issue or check the main project README!
